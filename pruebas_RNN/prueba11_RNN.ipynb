{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from src.maze import MazeEnv\n",
    "from tensorboardX import SummaryWriter\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perception(nn.Module):\n",
    "    def __init__(self, shape: tuple[int, int], action_space: int = 4) -> None:\n",
    "        super().__init__()\n",
    "        w, h = shape\n",
    "        k, p, s = 2, 1, 2\n",
    "        self.cn1 = nn.Conv2d(1, 8, k, s, p)  # 2\n",
    "        self.cn2 = nn.Conv2d(8, 32, k, s, p)  # 4\n",
    "\n",
    "        w, h = (w - k + 2 * p) // s + 1, (h - k + 2 * p) // s + 1\n",
    "        w, h = (w - k + 2 * p) // s + 1, (h - k + 2 * p) // s + 1\n",
    "        self.fc1 = nn.Linear(32 * w * h, action_space * 16)\n",
    "\n",
    "    def forward(self, img: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.relu(self.cn1(img))\n",
    "        x = F.relu(self.cn2(x))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Manager(nn.Module):\n",
    "    def __init__(self, dilation, num_actions, device):\n",
    "        super(Manager, self).__init__()\n",
    "\n",
    "        hidden_size = num_actions * 16\n",
    "\n",
    "        self.hx_memory = [torch.zeros(1, num_actions * 16).to(device) for _ in range(dilation)]\n",
    "\n",
    "        self.cx_memory = [torch.zeros(1, num_actions * 16).to(device) for _ in range(dilation)]\n",
    "        self.hidden_size = hidden_size\n",
    "        self.horizon = dilation\n",
    "        self.index = 0\n",
    "\n",
    "        self.fc = nn.Linear(num_actions * 16, num_actions * 16)\n",
    "        # todo: change lstm to dilated lstm\n",
    "        self.lstm = nn.LSTMCell(num_actions * 16, hidden_size=num_actions * 16)\n",
    "        # todo: add lstm initialization\n",
    "        self.lstm.bias_ih.data.fill_(0)\n",
    "        self.lstm.bias_hh.data.fill_(0)\n",
    "\n",
    "        self.fc_critic1 = nn.Linear(num_actions * 16, 50)\n",
    "        self.fc_critic2 = nn.Linear(50, 1)\n",
    "\n",
    "        self.fc_actor = nn.Linear(50, num_actions)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, (hx, cx) = inputs\n",
    "        x = F.relu(self.fc(x))\n",
    "        state = x\n",
    "\n",
    "        hx_t_1 = self.hx_memory[self.index]\n",
    "        cx_t_1 = self.cx_memory[self.index]\n",
    "        self.hx_memory[self.index] = hx\n",
    "        self.cx_memory[self.index] = cx\n",
    "\n",
    "        hx, cx = self.lstm(x, (hx_t_1, cx_t_1))\n",
    "        self.index += 1\n",
    "        if self.index >= self.horizon:\n",
    "            self.index %= self.horizon\n",
    "\n",
    "        goal = cx\n",
    "        value = F.relu(self.fc_critic1(goal))\n",
    "        value = self.fc_critic2(value)\n",
    "        \n",
    "        goal_norm = torch.norm(goal, p=2, dim=1).unsqueeze(1)\n",
    "        goal = goal / goal_norm.detach()\n",
    "        return goal, (hx, cx), value, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker(nn.Module):\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "        super(Worker, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTMCell(num_actions * 16, hidden_size=num_actions * 16)\n",
    "        self.lstm.bias_ih.data.fill_(0)\n",
    "        self.lstm.bias_hh.data.fill_(0)\n",
    "\n",
    "        # Linear projection of goal has no bias\n",
    "        self.fc = nn.Linear(num_actions * 16, 16, bias=False)\n",
    "\n",
    "        self.fc_critic1 = nn.Linear(num_actions * 16, 50)\n",
    "        self.fc_critic1_out = nn.Linear(50, 1)\n",
    "\n",
    "        self.fc_critic2 = nn.Linear(num_actions * 16, 50)\n",
    "        self.fc_critic2_out = nn.Linear(50, 1)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight, mode=\"fan_out\", nonlinearity=\"relu\")\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x, (hx, cx), goals = inputs\n",
    "        hx, cx = self.lstm(x, (hx, cx))\n",
    "\n",
    "        value_ext = F.relu(self.fc_critic1(hx))\n",
    "        value_ext = self.fc_critic1_out(value_ext)\n",
    "\n",
    "        value_int = F.relu(self.fc_critic2(hx))\n",
    "        value_int = self.fc_critic2_out(value_int)\n",
    "\n",
    "        worker_embed = hx.view(hx.size(0), self.num_actions, 16)\n",
    "\n",
    "        goals = goals.sum(dim=1)\n",
    "        # goals should be disconnected from Manager.\n",
    "        goal_embed = self.fc(goals.detach())\n",
    "        goal_embed = goal_embed.unsqueeze(-1)\n",
    "\n",
    "        policy = torch.bmm(worker_embed, goal_embed)\n",
    "        policy = policy.squeeze(-1)\n",
    "        policy = F.softmax(policy, dim=-1)\n",
    "        return policy, (hx, cx), value_ext, value_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FuN(nn.Module):\n",
    "    def __init__(self, shape, num_actions, horizon, device):\n",
    "        super(FuN, self).__init__()\n",
    "        self.horizon = horizon\n",
    "        self.device = device\n",
    "\n",
    "        self.percept = Perception(shape, num_actions)\n",
    "        self.manager = Manager(self.horizon, num_actions, device)\n",
    "        self.worker = Worker(num_actions)\n",
    "\n",
    "\n",
    "    def forward(self, x, m_lstm, w_lstm, goals_horizon):\n",
    "        percept_z = self.percept(x)\n",
    "\n",
    "        m_inputs = (percept_z, m_lstm)\n",
    "        goal, m_lstm, m_value, m_state = self.manager(m_inputs)\n",
    "        \n",
    "        # todo: at the start, there is no previous goals. Need to be checked\n",
    "        goals_horizon = torch.cat([goals_horizon[:, 1:], goal.unsqueeze(1)], dim=1)\n",
    "        \n",
    "        w_inputs = (percept_z, w_lstm, goals_horizon)\n",
    "        policy, w_lstm, w_value_ext, w_value_int = self.worker(w_inputs)\n",
    "        return policy, goal, goals_horizon, m_lstm, w_lstm, m_value, w_value_ext, w_value_int, m_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la transición para la memoria\n",
    "Transition = namedtuple('Transition', ('history', 'next_history', 'action', 'reward', 'goal', 'policy', 'm_lstm', 'w_lstm', 'm_value', 'w_value_ext', 'w_value_int', 'm_state'))\n",
    "\n",
    "# Definir la memoria\n",
    "class Memory(object):\n",
    "    def __init__(self):\n",
    "        self.memory = []\n",
    "\n",
    "    def push(self, history, next_history, action, reward, goal, policy, m_lstm, w_lstm, m_value, w_value_ext, w_value_int, m_state):\n",
    "        \"\"\"Saves a transition.\"\"\"\n",
    "        self.memory.append(Transition(history, next_history, action, reward, goal, policy, m_lstm, w_lstm, m_value, w_value_ext, w_value_int, m_state))\n",
    "\n",
    "    def sample(self):\n",
    "        transitions = Transition(*zip(*self.memory))\n",
    "        return transitions\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir la función de entrenamiento\n",
    "def train_model(net, optimizer, transitions):\n",
    "    # Desempaquetar las transiciones\n",
    "    history_batch = torch.cat(transitions.history)\n",
    "    next_history_batch = torch.cat(transitions.next_history)\n",
    "    action_batch = torch.cat(transitions.action)\n",
    "    reward_batch = torch.cat(transitions.reward)\n",
    "    goal_batch = torch.cat(transitions.goal)\n",
    "    policy_batch = torch.cat(transitions.policy)\n",
    "    m_lstm_batch = torch.cat(transitions.m_lstm)\n",
    "    w_lstm_batch = torch.cat(transitions.w_lstm)\n",
    "    m_value_batch = torch.cat(transitions.m_value)\n",
    "    w_value_ext_batch = torch.cat(transitions.w_value_ext)\n",
    "    w_value_int_batch = torch.cat(transitions.w_value_int)\n",
    "    m_state_batch = torch.cat(transitions.m_state)\n",
    "\n",
    "    # Calcular las pérdidas\n",
    "    policy_loss = -torch.mean(policy_batch * torch.log(policy_batch + 1e-5))\n",
    "    value_loss = torch.mean((reward_batch - m_value_batch) ** 2)\n",
    "    loss = policy_loss + value_loss\n",
    "\n",
    "    # Retropropagación y optimización\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    grad_norm = nn.utils.clip_grad_norm_(net.parameters(), 0.5)\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), grad_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image size: (17, 17)\n",
      "action size: 4\n"
     ]
    }
   ],
   "source": [
    "# Inicialización del entorno\n",
    "env = MazeEnv((8, 8), False, 3, True, render_mode=\"rgb-array\")\n",
    "initial = env.reset(False)  # Inicializar el laberinto\n",
    "plt.imshow(env.render())\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n",
    "\n",
    "H, W = initial.shape[0], initial.shape[1]\n",
    "img = torch.tensor(initial).reshape(1, 1, H, W).float()\n",
    "\n",
    "# Definir la red FuN\n",
    "num_actions = env.action_space.n\n",
    "img_shape = (H, W)\n",
    "print('image size:', img_shape)\n",
    "print('action size:', num_actions)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "horizon = 9\n",
    "\n",
    "net = FuN(img_shape, num_actions, horizon, device)\n",
    "optimizer = torch.optim.RMSprop(net.parameters(), lr=0.00025, eps=0.01)\n",
    "writer = SummaryWriter('logs')\n",
    "\n",
    "net.to(device)\n",
    "net.train()\n",
    "global_steps = 0\n",
    "score = 0\n",
    "count = 0\n",
    "grad_norm = 0\n",
    "\n",
    "# Inicializar histories con el estado inicial del entorno\n",
    "histories = torch.tensor(initial).reshape(1, 1, H, W).float().to(device)\n",
    "\n",
    "# Ajustar el tamaño del lote de las memorias LSTM\n",
    "batch_size = 1\n",
    "m_hx = torch.zeros(batch_size, num_actions * 16).to(device)\n",
    "m_cx = torch.zeros(batch_size, num_actions * 16).to(device)\n",
    "m_lstm = (m_hx, m_cx)\n",
    "\n",
    "w_hx = torch.zeros(batch_size, num_actions * 16).to(device)\n",
    "w_cx = torch.zeros(batch_size, num_actions * 16).to(device)\n",
    "w_lstm = (w_hx, w_cx)\n",
    "\n",
    "goals_horizon = torch.zeros(batch_size, horizon + 1, num_actions * 16).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[141], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m actions \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmultinomial(policies, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# send action to the environment and get state information\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m next_history, reward, done \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mactions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m next_history \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(next_history)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, H, W)\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32mc:\\Users\\danob\\OneDrive\\Desktop\\Master_2_semicuatrimestre\\Aprendizaje por refuerzo\\Aprendizaje_refuerzo\\src\\maze.py:218\u001b[0m, in \u001b[0;36mMazeEnv.step\u001b[1;34m(self, action, goal)\u001b[0m\n\u001b[0;32m    215\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;66;03m# apply movement\u001b[39;00m\n\u001b[1;32m--> 218\u001b[0m movement \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maction2direction\u001b[49m\u001b[43m[\u001b[49m\u001b[43maction\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;66;03m# the state of the env is the full maze. not the movement or place of the player\u001b[39;00m\n\u001b[0;32m    221\u001b[0m new_curr \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m+\u001b[39m movement[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_curr[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m+\u001b[39m movement[\u001b[38;5;241m1\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    count += 1\n",
    "    memory = Memory()\n",
    "    global_steps += 1\n",
    "\n",
    "    # gather samples from the environment\n",
    "    for i in range(5):\n",
    "        net_output = net(histories.to(device), m_lstm, w_lstm, goals_horizon)\n",
    "        policies, goal, goals_horizon, m_lstm, w_lstm, m_value, w_value_ext, w_value_int, m_state = net_output\n",
    "\n",
    "        actions = torch.multinomial(policies, 1).squeeze().cpu().numpy()\n",
    "\n",
    "        # send action to the environment and get state information\n",
    "        next_history, reward, done = env.step(actions)\n",
    "        next_history = torch.tensor(next_history).reshape(1, 1, H, W).float().to(device)\n",
    "\n",
    "        score += reward\n",
    "\n",
    "        # if agent in environment dies, print and log score\n",
    "        if done: \n",
    "            entropy = - policies * torch.log(policies + 1e-5)\n",
    "            entropy = entropy.mean().data.cpu()\n",
    "            print('global steps {} | score: {} | entropy: {:.4f} | grad norm: {:.3f} '.format(global_steps, score, entropy, grad_norm))\n",
    "            writer.add_scalar('log/score', score, global_steps)\n",
    "            score = 0\n",
    "\n",
    "        memory.push(histories, next_history, actions, reward, goal, policies, m_lstm, w_lstm, m_value, w_value_ext, w_value_int, m_state)\n",
    "        histories = next_history\n",
    "\n",
    "    # Train every 5 steps\n",
    "    if (global_steps % 5) == 0:\n",
    "        transitions = memory.sample()\n",
    "        loss, grad_norm = train_model(net, optimizer, transitions)\n",
    "        m_hx, m_cx = m_lstm\n",
    "        m_lstm = (m_hx.detach(), m_cx.detach())\n",
    "        w_hx, w_cx = w_lstm\n",
    "        w_lstm = (w_hx.detach(), w_cx.detach())\n",
    "        goals_horizon = goals_horizon.detach()\n",
    "\n",
    "    if count % 100 == 0:\n",
    "        ckpt_path = 'model.pt'\n",
    "        torch.save(net.state_dict(), ckpt_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
